{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "import os,sys\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics as mtrcs\n",
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import random\n",
    "import string\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from plotly import graph_objs as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import pickle\n",
    "\n",
    "def compute_ranks(scores, source_nodes, target_nodes, labels):\n",
    "    \"\"\"\n",
    "    Given source nodes, target nodes, and prediction score, compute the rank associated with every positive triple\n",
    "\n",
    "    :param predictionsDF: Dataframe of triplet, and prediction score\n",
    "    :return: Computed ranks for each positive triple\n",
    "    \"\"\"\n",
    "\n",
    "    ranks = {}\n",
    "    source_nodes_new = []\n",
    "\n",
    "    mask_neg = labels == \"N\"\n",
    "    scores_neg = scores[mask_neg]\n",
    "    sources_neg = source_nodes[mask_neg]\n",
    "    irx = np.argsort(-scores_neg)\n",
    "    scores_neg = scores_neg[irx]\n",
    "    sources_neg = sources_neg[irx]\n",
    "\n",
    "    scores_neg_dict = {}\n",
    "    sources_neg_dict = {}\n",
    "    for i, sn in tqdm(enumerate(sources_neg)):\n",
    "        v = sources_neg_dict.get(sn, [])\n",
    "        v.append(i)\n",
    "        sources_neg_dict[sn] = v\n",
    "    unique_sources = list(sources_neg_dict.keys())\n",
    "\n",
    "    mask_pos = np.logical_not(mask_neg)\n",
    "    scores_pos = scores[mask_pos]\n",
    "    sources_pos = source_nodes[mask_pos]\n",
    "    for i in tqdm(range(sum(mask_pos))):\n",
    "        score = scores_pos[i]\n",
    "        sourceNode = sources_pos[i]\n",
    "        scores_negative = scores_neg[sources_neg_dict[sourceNode]]\n",
    "        rank = np.sum(scores_negative > score) + 1\n",
    "\n",
    "        # detect ties and put the rank in the middle of the tie\n",
    "        irx = np.where(scores_negative == score)[0]\n",
    "        if len(irx) > 1:\n",
    "            rank = int((irx[0] + irx[-1]) / 2)\n",
    "\n",
    "        if sourceNode not in source_nodes_new:\n",
    "            ranks[sourceNode] = []\n",
    "            source_nodes_new.append(sourceNode)\n",
    "        ranks[sourceNode].append(rank)\n",
    "    return ranks\n",
    "\n",
    "def compute_precision_at_k(prediction_scores: pd.DataFrame, k: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    A function to compute precision@k from a dataframe with prediction scores containing scores for all triples.\n",
    "    IMPORTANT: Assumes that training triples have been removed from prediction_scores upstream. Also assumes that\n",
    "    scores for each gene for each disease are present in the dataframe (apart from scores for true training triples\n",
    "    that have been removed).\n",
    "\n",
    "    Precision@k is the fraction of true links in the test set in the top k predictions (when true links in the training\n",
    "    set have been removed). Therefore, it's meant to act as a kind of proxy for the fraction of true and novel links\n",
    "    we'd expect to see in the top k predictions for a disease.\n",
    "\n",
    "    :param prediction_scores: dataframe containing scores for all triples\n",
    "    :param k: k for which precision@k calculated\n",
    "    :return: dictionary with precision@k for each disease\n",
    "    \"\"\"\n",
    "    precision_per_disease = {}\n",
    "    disease_ids = list(set(prediction_scores[\"source_node\"]))\n",
    "\n",
    "    for disease_id in tqdm(disease_ids):\n",
    "        disease_id_scores = prediction_scores[\n",
    "            prediction_scores[\"source_node\"] == disease_id\n",
    "        ]\n",
    "        disease_id_scores_sorted = disease_id_scores.sort_values(\n",
    "            by=\"score\", ascending=False\n",
    "        )\n",
    "        top_k = disease_id_scores_sorted.iloc[:k]\n",
    "        number_true = len(top_k[top_k[\"label\"] == \"P\"])\n",
    "        disease_precision = number_true / k\n",
    "        precision_per_disease[disease_id] = disease_precision\n",
    "\n",
    "    return precision_per_disease\n",
    "\n",
    "def compute_precision_at_k_alt(\n",
    "    prediction_scores: pd.DataFrame, k: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    A function to compute precision@k from a dataframe with prediction scores containing scores for all triples.\n",
    "    IMPORTANT: Assumes that training triples have been removed from prediction_scores upstream. Also assumes that\n",
    "    scores for each gene for each disease are present in the dataframe (apart from scores for true training triples\n",
    "    that have been removed).\n",
    "\n",
    "    Precision@k is the fraction of true links in the test set in the top k predictions (when true links in the training\n",
    "    set have been removed). Therefore, it's meant to act as a kind of proxy for the fraction of true and novel links\n",
    "    we'd expect to see in the top k predictions for a disease.\n",
    "\n",
    "    :param prediction_scores: dataframe containing scores for all triples\n",
    "    :param k: k for which precision@k calculated\n",
    "    :return: dictionary with precision@k for each disease\n",
    "    \"\"\"\n",
    "    precision_per_disease = defaultdict(float)\n",
    "\n",
    "    # Group by disease_id and sort within each group\n",
    "    prediction_scores_sorted = prediction_scores.sort_values(\n",
    "        by=[\"source_node\", \"score\"], ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # Calculate precision for each disease_id\n",
    "    for disease_id, group in prediction_scores_sorted.groupby(\"source_node\"):\n",
    "        top_k_group = group.head(k)\n",
    "        number_true = len(top_k_group[top_k_group[\"label\"] == \"P\"])\n",
    "        disease_precision = number_true / k\n",
    "        precision_per_disease[disease_id] = disease_precision\n",
    "\n",
    "    return dict(precision_per_disease)\n",
    "\n",
    "def compute_hit_at_k_source_ranks(source_ranks, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute hit@k score from a triplet, positive or negative lablel and predicted score\n",
    "\n",
    "    :param k: parameter to be used for hit@k computation\n",
    "    :return: Average hit@k across all positive triples\n",
    "    \"\"\"\n",
    "    rank_average = []\n",
    "    for _, value in source_ranks.items():\n",
    "        hit_at_k_source = hit_at_k(value, k=k)\n",
    "        rank_average.append(hit_at_k_source)\n",
    "    # hit_at_k_score = np.round(float(sum(rank_average) / len(rank_average)), 3)\n",
    "    hit_at_k_score = float(sum(rank_average) / len(rank_average))\n",
    "    return hit_at_k_score\n",
    "\n",
    "def hit_at_k(ranks: list, k: int) -> float:\n",
    "    \"\"\"\n",
    "    hits@k describes the fraction of true entities that appear in the first k entities of the sorted rank list.\n",
    "\n",
    "    :param ranks: Calculated ranks\n",
    "    :param k: The maximum rank\n",
    "    :return: hit@k score\n",
    "    \"\"\"\n",
    "    ranks = np.asarray(ranks)\n",
    "    try:\n",
    "        htk = float(np.sum(ranks <= k) / len(ranks))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return htk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the prediction table\n",
    "link_to_predict = 'gene_mf'\n",
    "prediction_file = 'trained_models/'+link_to_predict+'/'+link_to_predict+'_predictions.tsv'\n",
    "df_predictions = pd.read_csv(prediction_file,sep='\\t',index_col=0)\n",
    "display(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction table for a particular gene\n",
    "gene = 'NODAL'\n",
    "#gene = 'OR2L13'\n",
    "#gene = \"SLC7A9\"\n",
    "display(df_predictions[df_predictions['GENE']==gene].sort_values(\"DotProduct\",ascending=False).head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'CDX2'\n",
    "display(df_predictions[df_predictions['GENE']==gene].sort_values(\"DotProduct\",ascending=False).head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUCs\n",
    "\n",
    "tab = df_predictions.copy()\n",
    "\n",
    "tab['Status'].replace('None', 0,inplace=True)\n",
    "tab['Status'].replace('New', 0,inplace=True)\n",
    "tab['Status'].replace(np.nan, 0,inplace=True)\n",
    "tab['Status'].replace('Train', 1,inplace=True)\n",
    "tab['Status'].replace('Test', 1,inplace=True)\n",
    "tab['Status'].replace('Validation', 1,inplace=True)\n",
    "\n",
    "\n",
    "feature = 'DotProduct'\n",
    "\n",
    "preds = list(tab[feature])\n",
    "labels = list(tab[\"Status\"])\n",
    "\n",
    "fpr, tpr, thresholds = mtrcs.roc_curve(labels, preds)\n",
    "\n",
    "# Using J-statistic: https://en.wikipedia.org/wiki/Youden%27s_J_statistic\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "print(\"Best Threshold=%f\" % (best_thresh))\n",
    "\n",
    "roc_auc = mtrcs.roc_auc_score(labels, preds)\n",
    "\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")  # diagonal roc curve of a random classifier\n",
    "plt.scatter(\n",
    "    fpr[ix], tpr[ix], marker=\"o\", color=\"black\", label=\"Best=%0.2f\" % best_thresh\n",
    ")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(f\"ROC curve for {feature}, \"+feature)\n",
    "plt.show()\n",
    "\n",
    "del(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute hit@k\n",
    "\n",
    "\n",
    "feature = \"DotProduct\"\n",
    "\n",
    "tab = df_predictions[['DISEASE','GENE',feature, \"Status\"]].copy()\n",
    "\n",
    "tab['Status'].replace('None', 0,inplace=True)\n",
    "tab['Status'].replace('New', 0,inplace=True)\n",
    "tab['Status'].replace(np.nan, 0,inplace=True)\n",
    "tab['Status'].replace('Train', 1,inplace=True)\n",
    "tab['Status'].replace('Test', 1,inplace=True)\n",
    "tab['Status'].replace('Validation', 1,inplace=True)\n",
    "\n",
    "tab.index = range(len(tab))\n",
    "print(np.sum(tab['Status']))\n",
    "\n",
    "direction = 'disease_gene'\n",
    "direction = 'gene_disease'\n",
    "\n",
    "if direction=='disease_gene':\n",
    "    tab.rename(\n",
    "        columns={\n",
    "            \"DISEASE\": \"source_node\",\n",
    "            feature: \"score\",\n",
    "            \"GENE\": \"target_node\",\n",
    "            \"Status\": \"label\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "else:\n",
    "    tab.rename(\n",
    "        columns={\n",
    "            \"GENE\": \"source_node\",\n",
    "            feature: \"score\",\n",
    "            \"DISEASE\": \"target_node\",\n",
    "            \"Status\": \"label\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "tab['label'].replace(0, \"N\",inplace=True)\n",
    "tab['label'].replace(1, \"P\",inplace=True)\n",
    "\n",
    "#display(tab)\n",
    "\n",
    "source_ranks = compute_ranks(np.array(tab['score']),\n",
    "                            np.array(tab['source_node']),\n",
    "                            np.array(tab['target_node']),\n",
    "                            np.array(tab['label']))\n",
    "k = 10\n",
    "hit_at_k_score = compute_hit_at_k_source_ranks(source_ranks, k=k)\n",
    "print(f\"hit_at_{k}_score={hit_at_k_score}\")\n",
    "#print(f'precision@{k}={compute_precision_at_k(tab,k=k)}')\n",
    "k = 100\n",
    "hit_at_k_score = compute_hit_at_k_source_ranks(source_ranks, k=k)\n",
    "print(f\"hit_at_{k}_score={hit_at_k_score}\")\n",
    "k = 1000\n",
    "hit_at_k_score = compute_hit_at_k_source_ranks(source_ranks, k=k)\n",
    "print(f\"hit_at_{k}_score={hit_at_k_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the precision@k\n",
    "#tab.to_csv(root_folder+'/data/temp.tsv',sep='\\t')\n",
    "k = 10\n",
    "precision_per_source = compute_precision_at_k_alt(tab,k)\n",
    "\n",
    "dfgene2hugo = pd.read_csv('tests/geneid2hugo.tsv',sep='\\t')\n",
    "gene2hugo = {str(row['GeneID']):row['HUGO'] for i,row in dfgene2hugo.iterrows()}\n",
    "hugo2geneid = {row['HUGO']:str(row['GeneID']) for i,row in dfgene2hugo.iterrows()}\n",
    "\n",
    "zeroconnected = []\n",
    "for disease_id, group in tqdm(tab.groupby(\"source_node\")):\n",
    "    number_true = len(group[group[\"label\"] == \"P\"])\n",
    "    if number_true<k:\n",
    "        zeroconnected.append(disease_id)\n",
    "print('Number of genes used to estimate precision:',len(precision_per_source)-len(zeroconnected))\n",
    "\n",
    "zeroconnected = set(zeroconnected)\n",
    "vals = [precision_per_source[k] for k in precision_per_source if not k in zeroconnected]\n",
    "print(f'Precision@{k}',np.mean(vals))\n",
    "plt.hist(vals,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evord-evognn-lib-JQhEMozN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
